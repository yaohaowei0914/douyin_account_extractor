import io
import json
import base64
import csv
import argparse
from datetime import datetime
import os
import glob
import re
from typing import List, Dict, Any, Tuple
import streamlit as st

def har_bytes_to_json(raw: bytes) -> Dict[str, Any] | None:
    """å°† HAR æ–‡ä»¶å­—èŠ‚è§£æä¸º JSON å¯¹è±¡ã€‚"""
    if not raw:
        return None
    try:
        text = raw.decode("utf-8", errors="ignore")
        return json.loads(text)
    except Exception:
        return None


def load_har_file(file_path):
    """Load and parse HAR file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"Successfully loaded HAR file with {len(data.get('log', {}).get('entries', []))} entries")
            return data
    except json.JSONDecodeError as e:
        print(f"Error parsing HAR file: {e}")
        return None
    except Exception as e:
        print(f"Error reading HAR file: {e}")
        return None

def decode_content(content):
    """Decode content with various encodings."""
    if not content:
        print("Content is empty")
        return None
        
    # If content is already a string, try to parse it as JSON
    if isinstance(content, str):
        try:
            return json.loads(content)
        except json.JSONDecodeError as e:
            print(f"Failed to parse string as JSON: {str(e)[:100]}")
    
    # Try different encodings for binary data
    encodings = ['utf-8', 'latin1', 'cp1252', 'ascii']
    
    # If it's base64 encoded
    if isinstance(content, str):
        try:
            # Try base64 decoding
            decoded = base64.b64decode(content)
            # Try different encodings for the decoded content
            for encoding in encodings:
                try:
                    text = decoded.decode(encoding)
                    try:
                        return json.loads(text)
                    except json.JSONDecodeError as e:
                        print(f"Failed to parse base64 decoded {encoding} text as JSON: {str(e)[:100]}")
                except UnicodeDecodeError as e:
                    print(f"Failed to decode base64 content with {encoding}: {str(e)[:100]}")
        except Exception as e:
            print(f"Failed to decode as base64: {str(e)[:100]}")
            
    # If it's raw binary
    if isinstance(content, (bytes, bytearray)):
        for encoding in encodings:
            try:
                text = content.decode(encoding)
                try:
                    return json.loads(text)
                except json.JSONDecodeError as e:
                    print(f"Failed to parse binary {encoding} text as JSON: {str(e)[:100]}")
            except UnicodeDecodeError as e:
                print(f"Failed to decode binary content with {encoding}: {str(e)[:100]}")
                
    print(f"All decoding attempts failed for content type: {type(content)}")
    if isinstance(content, str):
        print(f"First 100 chars of content: {content[:100]}")
    return None

def extract_simple_fields(obj, prefix=""):
    """Extract only simple fields (non-nested) from an object."""
    simple_fields = {}
    if not isinstance(obj, dict):
        return simple_fields
    
    for key, value in obj.items():
        field_name = f"{prefix}_{key}" if prefix else key
        # Only include simple types (not dict, list, or other complex types)
        if isinstance(value, (str, int, float, bool)) or value is None:
            simple_fields[field_name] = value
        elif isinstance(value, dict) and not value:  # Empty dict
            simple_fields[field_name] = ""
        elif isinstance(value, list):
            # Special handling: join non-empty list for specific keys like search_world
            if key == "search_world":
                try:
                    # Convert each element to string sensibly
                    def to_str(item):
                        if isinstance(item, (str, int, float, bool)) or item is None:
                            return "" if item is None else str(item)
                        if isinstance(item, dict):
                            # Try common fields, fallback to compact JSON
                            for k in ("word", "name", "title"):
                                if k in item and isinstance(item[k], (str, int, float)):
                                    return str(item[k])
                            return json.dumps(item, ensure_ascii=False)
                        return str(item)

                    joined = "ã€".join([to_str(x) for x in value]) if value else ""
                    simple_fields[field_name] = joined
                except Exception:
                    # Fallback: empty string on error
                    simple_fields[field_name] = ""
            else:
                # Keep behavior: only include empty list as empty string; skip non-empty lists
                if not value:
                    simple_fields[field_name] = ""
    
    return simple_fields

def extract_posts(data):
    posts = []
    
    try:
        if not isinstance(data, dict):
            print(f"Data is not a dictionary, type: {type(data)}")
            return posts
            
        if 'data' not in data:
            print(f"No 'data' key in response. Keys found: {list(data.keys())}")
            return posts
            
        data_section = data['data']
        
        # Look for 'list' in data section
        if 'list' not in data_section:
            print(f"No 'list' key in data section. Keys found: {list(data_section.keys())}")
            return posts
            
        item_list = data_section['list']
        if not isinstance(item_list, list):
            print(f"'list' is not a list, type: {type(item_list)}")
            return posts
            
        print(f"\nProcessing list with {len(item_list)} items")
        
        for i, item in enumerate(item_list):
            if not isinstance(item, dict):
                print(f"Item {i} is not a dictionary, skipping")
                continue
                
            post_data = {}
            
            # Extract aweme_info fields
            if 'aweme_info' in item and isinstance(item['aweme_info'], dict):
                aweme_fields = extract_simple_fields(item['aweme_info'], "aweme")
                post_data.update(aweme_fields)
                print(f"Extracted {len(aweme_fields)} aweme_info fields from item {i}")
            else:
                print(f"Item {i} missing or invalid aweme_info")
                
            # Extract author_info fields  
            if 'author_info' in item and isinstance(item['author_info'], dict):
                author_fields = extract_simple_fields(item['author_info'], "author")
                post_data.update(author_fields)
                print(f"Extracted {len(author_fields)} author_info fields from item {i}")
            else:
                print(f"Item {i} missing or invalid author_info")
                
            if post_data:  # Only add if we extracted some data
                posts.append(post_data)
                print(f"Added item {i} to collection with {len(post_data)} fields")
            else:
                print(f"No data extracted from item {i}")
                        
    except Exception as e:
        print(f"Error extracting posts: {str(e)}")
        
    return posts

def extract_data_from_har(har_data, config: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], int]:
    """éå† HAR entriesï¼Œè§£ç å“åº”ï¼Œæå–å¸–å­å¹¶æŒ‰å­—æ®µæ˜ å°„è¾“å‡ºã€‚"""
    extracted_data: List[Dict[str, Any]] = []
    valid_posts = 0

    if not har_data or 'log' not in har_data or 'entries' not in har_data['log']:
        return extracted_data, valid_posts

    entries = har_data['log']['entries']

    for entry in entries:
        try:
            if 'response' not in entry or 'content' not in entry['response']:
                continue
            content = entry['response']['content']
            if not content.get('text'):
                continue
            mime_type = content.get('mimeType', 'unknown')
            if mime_type == 'image/jpg':
                continue

            data = decode_content(content.get('text', ''))
            if not data:
                continue

            posts = extract_posts(data)
            for post in posts:
                valid_posts += 1
                post_data: Dict[str, Any] = {}
                
                # æå–æ‰€æœ‰å­—æ®µ
                all_fields = set()
                for p in [post]:
                    all_fields.update(p.keys())
                
                for field in sorted(all_fields):
                    post_data[field] = post.get(field, '')

                # è½¬æ¢æ—¶é—´æˆ³
                for field_name, value in post_data.items():
                    if 'time' in field_name.lower() and value and str(value).isdigit():
                        try:
                            timestamp = int(value)
                            post_data[field_name] = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                        except (ValueError, TypeError, OSError):
                            pass

                extracted_data.append(post_data)
        except Exception:
            continue

    return extracted_data, valid_posts


def extract_data(har_data):
    extracted_data = []
    total_entries = 0
    valid_posts = 0
    
    if not har_data or 'log' not in har_data or 'entries' not in har_data['log']:
        print("Invalid HAR data structure")
        print(f"Available keys: {list(har_data.keys()) if har_data else 'None'}")
        return extracted_data
        
    entries = har_data['log']['entries']
    print(f"\nProcessing {len(entries)} entries from HAR file")
    
    for entry in entries:
        total_entries += 1
        try:
            if 'response' not in entry or 'content' not in entry['response']:
                print(f"Skipping entry {total_entries}: No response content")
                continue
                
            content = entry['response']['content']
            if not content.get('text'):
                print(f"Skipping entry {total_entries}: No text content")
                continue
                
            # Skip image/jpg content
            mime_type = content.get('mimeType', 'unknown')
            if mime_type == 'image/jpg':
                print(f"Skipping entry {total_entries}: Image content (image/jpg)")
                continue
                
            print(f"\nProcessing entry {total_entries}:")
            print(f"Content type: {mime_type}")
            
            # Try to decode and parse the response
            data = decode_content(content.get('text', ''))
            if not data:
                print(f"Skipping entry {total_entries}: Could not decode content")
                continue
                
            print(f"Data keys: {list(data.keys())}")
                
            # Extract posts from the response
            posts = extract_posts(data)
            
            for post in posts:
                valid_posts += 1
                extracted_data.append(post)
                print(f"Added post with {len(post)} fields")
                
        except Exception as e:
            print(f"Error processing entry {total_entries}: {str(e)}")
            continue
            
    print(f"\nProcessed {total_entries} total entries")
    print(f"Found {valid_posts} valid posts")
    
    return extracted_data

def to_csv_bytes(rows: List[Dict[str, Any]]) -> bytes:
    """å°†ç»“æœè¡Œå†™æˆ CSVï¼ˆäºŒè¿›åˆ¶ï¼ŒUTF-8-SIGï¼‰ã€‚"""
    if not rows:
        return b''

    # è·å–æ‰€æœ‰å­—æ®µåå¹¶æ’åº
    all_fields = set()
    for r in rows:
        all_fields.update(r.keys())
    field_order = sorted(list(all_fields))

    buf = io.StringIO()
    writer = csv.DictWriter(buf, fieldnames=field_order, quoting=csv.QUOTE_ALL)
    writer.writeheader()
    writer.writerows(rows)
    return buf.getvalue().encode('utf-8-sig')


def save_to_csv(data, output_file):
    """Save extracted data to CSV file."""
    if not data:
        print("No data to save")
        return

    try:
        # Get all unique field names from all posts
        all_fields = set()
        for post in data:
            all_fields.update(post.keys())
        
        # Sort fields for consistent column order
        field_order = sorted(list(all_fields))
        
        # Convert Unix timestamps to Excel-compatible datetime strings for time-related fields
        for post in data:
            for field_name, value in post.items():
                if 'time' in field_name.lower() and value and str(value).isdigit():
                    try:
                        timestamp = int(value)
                        # Convert to Excel-compatible datetime string (YYYY-MM-DD HH:MM:SS)
                        post[field_name] = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    except (ValueError, TypeError, OSError):
                        # Keep original value if conversion fails
                        pass

        with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=field_order)
            writer.writeheader()
            writer.writerows(data)
        print(f"Successfully saved {len(data)} posts to {output_file}")
        print(f"CSV columns: {field_order}")
    except Exception as e:
        print(f"Error saving to CSV: {e}")

def process_directory(directory_path, output_file):
    """Process all HAR files in a directory and combine into a single CSV."""
    if not os.path.exists(directory_path):
        print(f"Directory not found: {directory_path}")
        return
        
    har_files = glob.glob(os.path.join(directory_path, "*.har"))
    if not har_files:
        print(f"No HAR files found in directory: {directory_path}")
        return
        
    print(f"Found {len(har_files)} HAR files to process")
    
    all_extracted_data = []
    
    for har_file in har_files:
        print(f"\nProcessing file: {har_file}")
        
        # Load and process the HAR file
        har_data = load_har_file(har_file)
        if not har_data:
            continue
            
        # Extract data from the HAR file
        extracted_data = extract_data(har_data)
        if extracted_data:
            all_extracted_data.extend(extracted_data)
            print(f"Added {len(extracted_data)} posts from {har_file}")
        else:
            print(f"No data extracted from {har_file}")
            
    if all_extracted_data:
        print(f"\nTotal posts collected: {len(all_extracted_data)}")
        save_to_csv(all_extracted_data, output_file)
    else:
        print("No data was extracted from any HAR files")

def main():
    parser = argparse.ArgumentParser(description='Extract Douyin posts from HAR file')
    parser.add_argument(
        '--har',
        help='Path to a HAR file or directory containing HAR files',
        default='./analyse_article/douyin_har_files/'
    )
    parser.add_argument(
        '--output',
        help='Output CSV file path',
        default='./analyse_article/douyin_posts.csv'
    )
    args = parser.parse_args()

    process_directory(args.har, args.output)


# æŒ‡å®šåˆ—æå–æ˜ å°„ï¼ˆä¸ extract_douyin_selected_columns.py ä¿æŒä¸€è‡´ï¼‰
SELECTED_HEADERS: List[Tuple[str, str]] = [
    ("author_author_id", "ID"),
    ("author_aweme_count", "è§†é¢‘ä¸ªæ•°"),
    ("author_comment_avg", "å¹³å‡è¯„è®º"),
    ("author_digg_avg", "å¹³å‡ç‚¹èµ"),
    ("author_follower_count", "ç²‰ä¸ä¸ªæ•°"),
    ("author_nickname", "name"),
    ("author_share_avg", "å¹³å‡åˆ†äº«"),
    ("author_unique_id", "æŠ–éŸ³å·"),
    ("aweme_aweme_cover", "è§†é¢‘å°é¢"),
    ("aweme_aweme_create_time", "åˆ›å»ºæ—¶é—´"),
    ("aweme_aweme_title", "æ ‡é¢˜"),
    ("aweme_aweme_url", "url"),
    ("aweme_collect_count", "æ”¶è—"),
    ("aweme_comment_count", "è¯„è®º"),
    ("aweme_digg_count", "ç‚¹èµ"),
    ("aweme_share_count", "åˆ†äº«"),
    ("aweme_play_count_v2", "é¢„ä¼°æ’­æ”¾"),
    ("aweme_sentence", "ç›¸å…³å…³é”®æ£€ç´¢"),
    ("aweme_search_world", "æœç´¢å…³é”®è¯"),
]

def extract_selected_columns(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """æŒ‰æŒ‡å®šåˆ—æ˜ å°„æå–å¹¶é‡å‘½ååˆ—"""
    selected: List[Dict[str, Any]] = []
    for row in rows:
        new_row = {}
        for src_key, dst_name in SELECTED_HEADERS:
            new_row[dst_name] = row.get(src_key, "")
        selected.append(new_row)
    return selected

def download_selected_csv(rows: List[Dict[str, Any]], filename: str) -> bytes:
    """å°†æŒ‡å®šåˆ—ç»“æœè½¬ä¸º UTF-8-SIG CSV å­—èŠ‚æµ"""
    selected = extract_selected_columns(rows)
    if not selected:
        return b""
    buf = io.StringIO()
    # æŒ‰ SELECTED_HEADERS é¡ºåºå†™åˆ—
    fieldnames = [dst for _, dst in SELECTED_HEADERS]
    writer = csv.DictWriter(buf, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
    writer.writeheader()
    writer.writerows(selected)
    return buf.getvalue().encode("utf-8-sig")

def streamlit_app():
    st.set_page_config(page_title="æŠ–éŸ³ HAR è½¬ CSV", page_icon="ğŸµ", layout="centered")
    st.title("ğŸµ æŠ–éŸ³ HAR è½¬ CSV")
    st.markdown(
        "ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª `.har` æ–‡ä»¶ï¼ˆä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„ **Network** é¢æ¿å¯¼å‡ºï¼‰ã€‚"
        "åº”ç”¨ä¼šæ‰«æå“åº”å†…å®¹ï¼Œæå–æŠ–éŸ³å¸–å­å­—æ®µå¹¶åˆå¹¶ä¸ºä¸€ä»½ CSVã€‚"
    )

    uploaded = st.file_uploader(
        "æ‹–æ‹½æˆ–é€‰æ‹© `.har` æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰",
        type=["har"],
        accept_multiple_files=True,
        help="Chrome/Edgeï¼šæ‰“å¼€å¼€å‘è€…å·¥å…· â†’ Network â†’ å³é”®ç©ºç™½å¤„ â†’ Save all as HAR with contentã€‚"
    )
    st.subheader("ğŸ“¦ æŒ‡å®šåˆ—å¯¼å‡º")

    gen_btn = st.button("ç”ŸæˆæŒ‡å®šåˆ— CSV")

    if gen_btn:
        if not uploaded:
            st.warning("è¯·è‡³å°‘ä¸Šä¼  1 ä¸ª `.har` æ–‡ä»¶ã€‚")
            st.stop()

        all_rows: List[Dict[str, Any]] = []
        progress = st.progress(0)

        # é…ç½®ï¼ˆç©ºé…ç½®ï¼Œå› ä¸ºæŠ–éŸ³ç‰ˆæœ¬ç›´æ¥æå–æ‰€æœ‰å­—æ®µï¼‰
        config = {'fields': {}}

        for i, uf in enumerate(uploaded, start=1):
            with st.status(f"æ­£åœ¨å¤„ç† **{uf.name}** â€¦", expanded=False):
                raw = uf.read()
                har_obj = har_bytes_to_json(raw)
                if not har_obj:
                    st.error(f"æ— æ³•è§£æ HARï¼š{uf.name}")
                else:
                    rows, count = extract_data_from_har(har_obj, config)
                    all_rows.extend(rows)
                    st.write(f"åœ¨è¯¥æ–‡ä»¶ä¸­å‘ç° **{count}** æ¡å¸–å­ã€‚")

            progress.progress(i / len(uploaded))

        if not all_rows:
            st.info("æœªåœ¨æ‰€ä¸Šä¼ çš„æ–‡ä»¶ä¸­æ‰¾åˆ°å¯ç”¨çš„å¸–å­æ•°æ®ã€‚")
            st.stop()

        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_name = f"douyin_posts_{ts}.csv"

        selected_csv_bytes = download_selected_csv(all_rows, out_name)

        st.success(f"å®Œæˆï¼å…±ä» **{len(uploaded)}** ä¸ªæ–‡ä»¶æå– **{len(all_rows)}** æ¡è®°å½•ï¼Œå¹¶ç”ŸæˆæŒ‡å®šåˆ— CSVã€‚")
        st.download_button(
            "â¬‡ï¸ ä¸‹è½½æŒ‡å®šåˆ— CSV",
            data=selected_csv_bytes,
            file_name=out_name,
            mime="text/csv"
        )
        st.caption("åˆ—é¡ºåºï¼šIDã€è§†é¢‘ä¸ªæ•°ã€å¹³å‡è¯„è®ºã€å¹³å‡ç‚¹èµã€ç²‰ä¸ä¸ªæ•°ã€nameã€å¹³å‡åˆ†äº«ã€æŠ–éŸ³å·ã€è§†é¢‘å°é¢ã€åˆ›å»ºæ—¶é—´ã€æ ‡é¢˜ã€urlã€æ”¶è—ã€è¯„è®ºã€ç‚¹èµã€åˆ†äº«ã€é¢„ä¼°æ’­æ”¾ã€ç›¸å…³å…³é”®æ£€ç´¢ã€æœç´¢å…³é”®è¯")


if __name__ == '__main__':
    # æ£€æŸ¥æ˜¯å¦åœ¨Streamlitç¯å¢ƒä¸­è¿è¡Œ
    try:
        # å¦‚æœèƒ½å¯¼å…¥streamlitä¸”åœ¨streamlitç¯å¢ƒä¸­ï¼Œè¿è¡Œstreamlitåº”ç”¨
        import sys
        if 'streamlit' in sys.modules or any('streamlit' in arg for arg in sys.argv):
            streamlit_app()
        else:
            main()
    except:
        main()
